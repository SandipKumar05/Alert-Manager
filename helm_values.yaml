alertmanager:
  alertmanagerSpec:
    config:
      global:
        resolve_timeout: 5m
      route:
        group_by: ['alertname']
        group_wait: 30s
        group_interval: 5m
        repeat_interval: 1h
        receiver: 'slack-notifications'
      receivers:
        - name: 'slack-notifications'
          slack_configs:
            - api_url: 'https://hooks.slack.com/services/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'
              channel: '#demo'
              send_resolved: true
              title: '{{ template "slack.default.title" . }}'
              text: '{{ template "slack.default.text" . }}'
      templates:
        - '/etc/alertmanager/template/*.tmpl'

server:
  config:
    global:
      scrape_interval: 15s
    alerting:
      alertmanagers:
        - static_configs:
            - targets: ['prometheus-kube-prometheus-alertmanager:9093']
    rule_files:
      - /etc/config/alertmanager_rules.yml

  extraScrapeConfigs: |-
    - job_name: 'alertmanager'
      static_configs:
        - targets: ['prometheus-kube-prometheus-alertmanager:9093']

  extraArgs:
    config.file: /etc/config/prometheus.yml

  extraConfigmapMounts:
    - name: custom-prometheus-config
      mountPath: /etc/config
      configMap: custom-prometheus-config

extraConfigmapMounts:
  - name: custom-prometheus-config
    mountPath: /etc/config
    configMap: custom-prometheus-config

# Example ConfigMap for custom configuration
customConfig:
  custom-prometheus-config:
    prometheus.yml: |-
      global:
        scrape_interval: 15s
      alerting:
        alertmanagers:
          - static_configs:
              - targets: ['prometheus-kube-prometheus-alertmanager:9093']
      rule_files:
        - /etc/config/alertmanager_rules.yml
    alertmanager_rules.yml: |-
      groups:
        - name: alertmanager.rules
          rules:
            - alert: AlertManagerDown
              expr: up{job="alertmanager"} == 0
              for: 5m
              labels:
                severity: critical
              annotations:
                summary: "Alert Manager is down"
                description: "The Alert Manager instance is down for more than 5 minutes."

            - alert: HighErrorRate
              expr: rate(alertmanager_http_requests_total{status=~"5.."}[5m]) > 0.05
              for: 5m
              labels:
                severity: warning
              annotations:
                summary: "High error rate in Alert Manager"
                description: "There is a high rate of 5xx errors in the Alert Manager over the last 5 minutes."

            - alert: HighLatency
              expr: histogram_quantile(0.95, sum(rate(alertmanager_http_request_duration_seconds_bucket[5m])) by (le)) > 0.5
              for: 5m
              labels:
                severity: warning
              annotations:
                summary: "High request latency in Alert Manager"
                description: "The 95th percentile request latency for the Alert Manager is greater than 0.5 seconds over the last 5 minutes."
